{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying and Monitoring\n",
    "\n",
    "In this notebook we will deploy the network traffic classification model that we have trained in the previous steps to Amazon SageMaker hosting, which will expose a fully-managed real-time endpoint to execute inferences.\n",
    "\n",
    "Amazon SageMaker is adding new capabilities that monitor ML models while in production and detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). They enable you to capture the metadata and the input and output for invocations of the models that you deploy with Amazon SageMaker. They also enable you to analyze the data and monitor its quality. \n",
    "\n",
    "We will deploy the model to a real-time endpoint with data capture enabled and start collecting some inference inputs/outputs. Then, we will create a baseline and finally enable model monitoring to compare inference data with respect to the baseline and analyze the quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set some variables, including the AWS region we are working in, the IAM execution role of the notebook instance and the Amazon S3 bucket where we will store data and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'aim362'\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment with Data Capture\n",
    "\n",
    "We are going to deploy the latest network traffic classification model that we have trained. To deploy a model using the SM Python SDK, we need to make sure we have the Amazon S3 URI where the model artifacts are stored and the URI of the Docker container that will be used for hosting this model.\n",
    "\n",
    "First, let's determine the Amazon S3 URI of the model artifacts by using a couple of utility functions which query Amazon SageMaker service to get the latest training job whose name starts with 'nw-traffic-classification-xgb' and then describing the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_latest_training_job_name(base_job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.list_training_jobs(NameContains=base_job_name, SortBy='CreationTime', \n",
    "                                         SortOrder='Descending', StatusEquals='Completed')\n",
    "    if len(response['TrainingJobSummaries']) > 0 :\n",
    "        return response['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    else:\n",
    "        raise Exception('Training job not found.')\n",
    "\n",
    "def get_training_job_s3_model_artifacts(job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.describe_training_job(TrainingJobName=job_name)\n",
    "    s3_model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    return s3_model_artifacts\n",
    "\n",
    "latest_training_job_name = get_latest_training_job_name('nw-traffic-classification-xgb')\n",
    "print(latest_training_job_name)\n",
    "model_path = get_training_job_s3_model_artifacts(latest_training_job_name)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we are going to use the same XGBoost Docker container we used for training, which also offers inference capabilities. As a consequence, we can just create the XGBoostModel object of the Amazon SageMaker Python SDK and then invoke its .deploy() method to execute deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also provide an entrypoint script to be invoked at deployment/inference time. The purpose of this code is deserializing and loading the XGB model. In addition, we are re-defining the output functions as we want to extract the class value from the default array output. For example, for class 3 the XGB container would output [3.] but we want to extract only the 3 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source_dir/deploy_xgboost.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the XGBoostModel object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.xgboost import XGBoostModel\n",
    "\n",
    "model_name = 'nw-traffic-classification-xgb-model-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, prefix)\n",
    "xgboost_model = XGBoostModel(model_data=model_path,\n",
    "                             entry_point='deploy_xgboost.py',\n",
    "                             source_dir='source_dir/',\n",
    "                             name=model_name,\n",
    "                             code_location=code_location,\n",
    "                             framework_version='0.90-2',\n",
    "                             role=role, \n",
    "                             sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create an endpoint with data capture enabled, for monitoring the model data quality.\n",
    "Data capture is enabled at enpoint configuration level for the Amazon SageMaker real-time endpoint. You can choose to capture the request payload, the response payload or both and captured data is stored in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(bucket_name, prefix)\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "endpoint_name = 'nw-traffic-classification-xgb-ep-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "\n",
    "pred = xgboost_model.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m5.xlarge',\n",
    "                            endpoint_name=endpoint_name,\n",
    "                            data_capture_config=DataCaptureConfig(\n",
    "                                enable_capture=True,\n",
    "                                sampling_percentage=100,\n",
    "                                destination_s3_uri=s3_capture_upload_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the deployment has been completed, we can leverage on the RealTimePredictor object to execute HTTPs requests against the deployed endpoint and get inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "pred = Predictor(endpoint_name)\n",
    "pred.serializer = CSVSerializer()\n",
    "\n",
    "# Expecting class 4\n",
    "test_values = \"80,1056736,3,4,20,964,20,0,6.666666667,11.54700538,964,0,241.0,482.0,931.1691850999999,6.6241710320000005,176122.6667,\\\n",
    "431204.4454,1056315,2,394,197.0,275.77164469999997,392,2,1056733,352244.3333,609743.1115,1056315,24,0,0,0,0,72,92,\\\n",
    "2.8389304419999997,3.78524059,0,964,123.0,339.8873763,115523.4286,0,0,1,1,0,0,0,1,1.0,140.5714286,6.666666667,\\\n",
    "241.0,0.0,0.0,0.0,0.0,0.0,0.0,3,20,4,964,8192,211,1,20,0.0,0.0,0,0,0.0,0.0,0,0,20,2,2018,1,0,1,0\"\n",
    "\n",
    "result = pred.predict(test_values)\n",
    "print(result)\n",
    "\n",
    "# Expecting class 7\n",
    "test_values = \"80,10151,2,0,0,0,0,0,0.0,0.0,0,0,0.0,0.0,0.0,197.0249237,10151.0,0.0,10151,10151,10151,10151.0,0.0,10151,10151,0,0.0,\\\n",
    "0.0,0,0,0,0,0,0,40,0,197.0249237,0.0,0,0,0.0,0.0,0.0,0,0,0,0,1,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2,0,0,0,32738,\\\n",
    "-1,0,20,0.0,0.0,0,0,0.0,0.0,0,0,21,2,2018,2,0,1,0\"\n",
    "\n",
    "result = pred.predict(test_values)\n",
    "print(result)\n",
    "\n",
    "# Expecting class 0\n",
    "test_values = \"80,54322832,2,0,0,0,0,0,0.0,0.0,0,0,0.0,0.0,0.0,0.0368169318,54322832.0,0.0,54322832,54322832,54322832,54322832.0,0.0,\\\n",
    "54322832,54322832,0,0.0,0.0,0,0,0,0,0,0,40,0,0.0368169318,0.0,0,0,0.0,0.0,0.0,0,0,0,0,1,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,\\\n",
    "0.0,0.0,0.0,0.0,2,0,0,0,279,-1,0,20,0.0,0.0,0,0,0.0,0.0,0,0,23,2,2018,4,0,1,0\"\n",
    "\n",
    "result = pred.predict(test_values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred.\n",
    "\n",
    "**Note that the delivery of capture data to Amazon S3 can require a couple of minutes so next cell might error. If this happens, please retry after a minute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/monitoring/datacapture/{}'.format(prefix, endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the contents of one of these files and see how capture records are organized in JSON lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {capture_files[0]} datacapture/captured_data_example.jsonl\n",
    "!head datacapture/captured_data_example.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can better understand the content of each JSON line like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"datacapture/captured_data_example.jsonl\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each inference request, we get input data, output data and some metadata like the inference time captured and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our validation dataset let's ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics for our features. Note that we are using the validation dataset for this workshop to make sure baselining time is short, and that file extension needs to be changed since the baselining jobs require .CSV file extension as default.\n",
    "In reality, you might be willing to use a larger dataset as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket_key_prefix = \"aim362/data/val/\"\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "for s3_object in bucket.objects.filter(Prefix=bucket_key_prefix):\n",
    "    target_key = s3_object.key.replace('data/val/', 'monitoring/baselining/data/').replace('.part', '.csv')\n",
    "    print('Copying {0} to {1} ...'.format(s3_object.key, target_key))\n",
    "    \n",
    "    copy_source = {\n",
    "        'Bucket': bucket_name,\n",
    "        'Key': s3_object.key\n",
    "    }\n",
    "    s3.Bucket(bucket_name).copy(copy_source, target_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_path = 's3://{0}/{1}/monitoring/baselining/data'.format(bucket_name, prefix)\n",
    "baseline_results_path = 's3://{0}/{1}/monitoring/baselining/results'.format(bucket_name, prefix)\n",
    "\n",
    "print(baseline_data_path)\n",
    "print(baseline_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that running the baselining job will require 8-10 minutes. In the meantime, you can take a look at the Deequ library, used to execute these analyses with the default Model Monitor container: https://github.com/awslabs/deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.4xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_path,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_path,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the statistics that were generated by the baselining job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can also visualize the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The baselining job has inspected the validation dataset and generated constraints and statistics, that will be used to monitor our endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating violations artificially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get some result relevant to monitoring analysis, we are going to generate artificially some inferences with feature values causing specific violations, and then invoke the endpoint with this data.\n",
    "\n",
    "This requires about 2 minutes for 1000 inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "dist_values = np.random.normal(1, 0.2, 1000)\n",
    "\n",
    "# Tot Fwd Pkts -> set to float (expected integer) [second feature]\n",
    "# Flow Duration -> set to empty (missing value) [third feature]\n",
    "# Fwd Pkt Len Mean -> sampled from random normal distribution [nineth feature]\n",
    "\n",
    "artificial_values = \"22,,40.3,0,0,0,0,0,{0},0.0,0,0,0.0,0.0,0.0,0.0368169318,54322832.0,0.0,54322832,54322832,54322832,54322832.0,0.0,\\\n",
    "54322832,54322832,0,0.0,0.0,0,0,0,0,0,0,40,0,0.0368169318,0.0,0,0,0.0,0.0,0.0,0,0,0,0,1,0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,\\\n",
    "0.0,0.0,0.0,0.0,2,0,0,0,279,-1,0,20,0.0,0.0,0,0,0.0,0.0,0,0,23,2,2018,4,0,1,0\"\n",
    "\n",
    "for i in range(1000):\n",
    "    pred.predict(artificial_values.format(str(dist_values[i])))\n",
    "    time.sleep(0.15)\n",
    "    if i > 0 and i % 100 == 0 :\n",
    "        print('Executed {0} inferences.'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have built the baseline for our data, we can enable endpoint monitoring by creating a monitoring schedule.\n",
    "When the schedule fires, a monitoring job will be kicked-off and will inspect the data captured at the endpoint with respect to the baseline; then it will generate some report files that can be used to analyze monitoring results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the monitoring schedule for the previously created endpoint. When we create the schedule, we can also specify two scripts that will preprocess the records before the analysis takes place and execute post-processing at the end.\n",
    "For this example, we are not going to use a record preprocessor, and we are just specifying a post-processor that outputs some text for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize postprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the script to Amazon S3 and specify the path where the monitoring reports will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "monitoring_code_prefix = '{0}/monitoring/code'.format(prefix)\n",
    "print(monitoring_code_prefix)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(monitoring_code_prefix + '/postprocessor.py').upload_file('postprocessor.py')\n",
    "postprocessor_path = 's3://{0}/{1}/monitoring/code/postprocessor.py'.format(bucket_name, prefix)\n",
    "print(postprocessor_path)\n",
    "\n",
    "reports_path = 's3://{0}/{1}/monitoring/reports'.format(bucket_name, prefix)\n",
    "print(reports_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the monitoring schedule with hourly schedule execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_name = pred.endpoint\n",
    "\n",
    "mon_schedule_name = 'nw-traffic-classification-xgb-mon-sch-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    post_analytics_processor_script=postprocessor_path,\n",
    "    output_s3_uri=reports_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "desc_schedule_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Monitoring Schedule\n",
    "\n",
    "Once the schedule is created, it will kick of jobs at specified intervals. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. \n",
    "You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. Since we don't want to wait for the hour in this example we can delete the schedule and use the code in next steps to simulate what will happen when a schedule is triggered, by running an Amazon SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is just for the purpose of running this example.\n",
    "my_default_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggering execution manually\n",
    "\n",
    "In oder to trigger the execution manually, we first get all paths to data capture, baseline statistics, baseline constraints, etc.\n",
    "Then, we use a utility fuction, defined in <a href=\"./monitoringjob_utils.py\">monitoringjob_utils.py</a>, to run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "data_capture_path = capture_files[len(capture_files) - 1][: capture_files[len(capture_files) - 1].rfind('/')]\n",
    "statistics_path = baseline_results_path + '/statistics.json'\n",
    "constraints_path = baseline_results_path + '/constraints.json'\n",
    "\n",
    "print(data_capture_path)\n",
    "print(postprocessor_path)\n",
    "print(statistics_path)\n",
    "print(constraints_path)\n",
    "print(reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoringjob_utils import run_model_monitor_job_processor\n",
    "\n",
    "run_model_monitor_job_processor(region, 'ml.m5.xlarge', role, data_capture_path, statistics_path, constraints_path, reports_path,\n",
    "                                postprocessor_path=postprocessor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the monitoring job completes, monitoring reports are saved to Amazon S3. Let's list the generated reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "monitoring_reports_prefix = '{}/monitoring/reports/{}'.format(prefix, pred.endpoint)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket_name, Prefix=monitoring_reports_prefix)\n",
    "try:\n",
    "    monitoring_reports = ['s3://{0}/{1}'.format(bucket_name, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "    print(\"Monitoring Reports Files: \")\n",
    "    print(\"\\n \".join(monitoring_reports))\n",
    "except:\n",
    "    print('No monitoring reports found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then copy monitoring reports locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {monitoring_reports[0]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[1]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[2]} monitoring/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the violations identified by the monitoring execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "file = open('monitoring/constraint_violations.json', 'r')\n",
    "data = file.read()\n",
    "\n",
    "violations_df = pd.io.json.json_normalize(json.loads(data)['violations'])\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the violations identified correspond to the ones that we artificially generated and that there is a feature that is generating some drift from the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking yourself what are the type of violations that are monitored and how drift from the baseline is computed.\n",
    "\n",
    "The types of violations monitored are listed here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html. Most of them use configurable thresholds, that are specified in the monitoring configuration section of the baseline constraints JSON. Let's take a look at this configuration from the baseline constraints file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {statistics_path} baseline/\n",
    "!aws s3 cp {constraints_path} baseline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"baseline/constraints.json\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data)['monitoring_config'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration is intepreted when the monitoring job is executed and used to compare captured data to the baseline. If you want to customize this section, you will have to update the **constraints.json** file and upload it back to Amazon S3 before launching the monitoring job.\n",
    "\n",
    "When data distributions are compared to detect potential drift, you can choose to use either a _Simple_ or _Robust_ comparison method, where the latter has to be preferred when dealing with small datasets. Additional info: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can delete the endpoint to free-up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.delete_endpoint()\n",
    "pred.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "A Realistic Cyber Defense Dataset (CSE-CIC-IDS2018) https://registry.opendata.aws/cse-cic-ids2018/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
